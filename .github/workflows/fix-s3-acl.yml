name: ğŸ›¡ï¸ S3 Security Gatekeeper (Debug)

on:
  push:
    branches: [ main, develop ]
    paths:
      - '**/*.tf'
      - '**/*.json'
      - '.github/workflows/s3-security.yml'
  pull_request:
    branches: [ main ]
    paths:
      - '**/*.tf'
      - '**/*.json'
  workflow_dispatch:
    inputs:
      scan_all:
        description: 'Scan all files (not just changed)'
        required: false
        default: 'false'
        type: boolean
      auto_fix:
        description: 'Auto-generate fixes for S3 issues'
        required: false
        default: 'true'
        type: boolean
      debug_mode:
        description: 'Enable debug output'
        required: false
        default: 'true'
        type: boolean

env:
  CHECKOV_VERSION: "3.2.0"
  PYTHON_VERSION: "3.11"
  DEBUG: ${{ github.event.inputs.debug_mode || 'true' }}

jobs:
  # Job 1: Run Checkov Security Scan
  security-scan:
    name: ğŸ” Security Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
      pull-requests: write
    
    outputs:
      has-s3-issues: ${{ steps.check-s3-issues.outputs.has-issues }}
      scan-status: ${{ steps.checkov-scan.outputs.status }}
      s3-issues-count: ${{ steps.check-s3-issues.outputs.issues-count }}
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install Checkov
        run: |
          pip install checkov==${{ env.CHECKOV_VERSION }}
          checkov --version

      - name: ğŸ“ Create Reports Directory
        run: mkdir -p checkov_reports scripts

      - name: ğŸ” Debug - List Terraform Files
        if: env.DEBUG == 'true'
        run: |
          echo "=== Terraform files found ==="
          find . -name "*.tf" -type f | head -20
          echo "=== File count ==="
          find . -name "*.tf" -type f | wc -l

      - name: ğŸ” Run Checkov Scan
        id: checkov-scan
        continue-on-error: true
        run: |
          echo "ğŸ” Running Checkov security scan..."
          
          # Run Checkov with JSON output
          checkov \
            --framework terraform \
            --output json \
            --output-file checkov_reports/report.json \
            --directory . \
            --skip-check CKV_TF_1 \
            --verbose \
            || echo "checkov_exit_code=$?" >> $GITHUB_OUTPUT
          
          # Also generate CLI output for logs
          echo "=== Checkov CLI Output ==="
          checkov \
            --framework terraform \
            --directory . \
            --compact || true
          
          echo "status=completed" >> $GITHUB_OUTPUT

      - name: ğŸ” Debug - Show Checkov Report
        if: env.DEBUG == 'true'
        run: |
          echo "=== Checkov Report Contents ==="
          if [ -f "checkov_reports/report.json" ]; then
            echo "Report file exists, size: $(wc -c < checkov_reports/report.json) bytes"
            echo "First 2000 characters:"
            head -c 2000 checkov_reports/report.json
            echo -e "\n=== Report structure ==="
            python3 -c "
          import json
          try:
              with open('checkov_reports/report.json', 'r') as f:
                  data = json.load(f)
              print('Report type:', type(data))
              if isinstance(data, dict):
                  print('Keys:', list(data.keys()))
                  if 'results' in data:
                      results = data['results']
                      print('Results type:', type(results))
                      if isinstance(results, dict):
                          print('Results keys:', list(results.keys()))
                          if 'failed_checks' in results:
                              print('Failed checks count:', len(results['failed_checks']))
                          if 'passed_checks' in results:
                              print('Passed checks count:', len(results['passed_checks']))
              elif isinstance(data, list):
                  print('List length:', len(data))
          except Exception as e:
              print('Error parsing report:', e)
          "
          else
            echo "âŒ Report file not found!"
            ls -la checkov_reports/
          fi

      - name: ğŸ“Š Check for S3 Issues
        id: check-s3-issues
        run: |
          cat > scripts/check_s3_issues.py << 'EOF'
          import json
          import sys
          from pathlib import Path
          
          def main():
              report_path = Path("checkov_reports/report.json")
              if not report_path.exists():
                  print("âŒ No Checkov report found", file=sys.stderr)
                  print("has-issues=false")
                  print("issues-count=0")
                  return
              
              try:
                  with open(report_path, 'r') as f:
                      data = json.load(f)
                  
                  failed_checks = []
                  
                  # Handle different Checkov report formats
                  if isinstance(data, dict):
                      if 'results' in data:
                          results = data['results']
                          if isinstance(results, dict) and 'failed_checks' in results:
                              failed_checks = results['failed_checks']
                          elif isinstance(results, list):
                              for result in results:
                                  if isinstance(result, dict) and 'failed_checks' in result:
                                      failed_checks.extend(result['failed_checks'])
                      elif 'failed_checks' in data:
                          failed_checks = data['failed_checks']
                  elif isinstance(data, list):
                      for item in data:
                          if isinstance(item, dict):
                              if 'results' in item and 'failed_checks' in item['results']:
                                  failed_checks.extend(item['results']['failed_checks'])
                              elif 'failed_checks' in item:
                                  failed_checks.extend(item['failed_checks'])
                  
                  print(f"Total failed checks found: {len(failed_checks)}", file=sys.stderr)
                  
                  # S3 security checks we're targeting
                  s3_checks = [
                      'CKV_AWS_20',  # S3 Bucket has an ACL defined which allows public access
                      'CKV2_AWS_6',  # S3 bucket has S3 Bucket Public Access Block enabled
                      'CKV_AWS_21',  # S3 bucket has server-side encryption enabled
                      'CKV_AWS_18',  # S3 bucket has access logging enabled
                      'CKV_AWS_19'   # S3 bucket has MFA delete enabled
                  ]
                  
                  s3_issues = []
                  for check in failed_checks:
                      check_id = check.get('check_id', '')
                      if check_id in s3_checks:
                          s3_issues.append(check)
                          print(f"Found S3 issue: {check_id} in {check.get('file_path', 'unknown')}", file=sys.stderr)
                  
                  print(f"S3 issues found: {len(s3_issues)}", file=sys.stderr)
                  print("has-issues=true" if s3_issues else "has-issues=false")
                  print(f"issues-count={len(s3_issues)}")
                  
                  # Debug: show all check IDs found
                  all_check_ids = set(check.get('check_id', '') for check in failed_checks)
                  print(f"All check IDs found: {sorted(all_check_ids)}", file=sys.stderr)
                  
              except Exception as e:
                  print(f"Error processing report: {e}", file=sys.stderr)
                  print("has-issues=false")
                  print("issues-count=0")
          
          if __name__ == "__main__":
              main()
          EOF
          
          # Run the check and capture outputs
          OUTPUT=$(python scripts/check_s3_issues.py)
          echo "$OUTPUT" >> $GITHUB_OUTPUT
          echo "ğŸ” S3 Issues Check Results:"
          echo "$OUTPUT"

      - name: ğŸ“¤ Upload Scan Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: checkov-scan-results
          path: checkov_reports/
          retention-days: 30

  # Job 2: Generate Reports and Fixes
  generate-fixes:
    name: ğŸ”§ Generate S3 Fixes
    runs-on: ubuntu-latest
    needs: security-scan
    if: needs.security-scan.outputs.has-s3-issues == 'true'
    permissions:
      contents: write
      pull-requests: write
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“ Create Scripts Directory
        run: mkdir -p scripts

      - name: ğŸ“¥ Download Scan Results
        uses: actions/download-artifact@v4
        with:
          name: checkov-scan-results
          path: checkov_reports/

      - name: ğŸ” Debug - Verify Downloaded Files
        if: env.DEBUG == 'true'
        run: |
          echo "=== Downloaded files ==="
          ls -la checkov_reports/
          if [ -f "checkov_reports/report.json" ]; then
            echo "Report size: $(wc -c < checkov_reports/report.json) bytes"
          fi

      - name: ğŸ“ Generate Markdown Report
        run: |
          cat > scripts/generate_reports.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          
          def load_checkov_report():
              report_path = Path("checkov_reports/report.json")
              if not report_path.exists():
                  print("âŒ No Checkov report found")
                  return None
              try:
                  with open(report_path, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Error loading report: {e}")
                  return None
          
          def parse_checkov_data(data):
              failed_checks = []
              if isinstance(data, dict):
                  if "results" in data:
                      results = data["results"]
                      if isinstance(results, dict) and "failed_checks" in results:
                          failed_checks = results["failed_checks"]
                      elif isinstance(results, list):
                          for result in results:
                              if isinstance(result, dict) and "failed_checks" in result:
                                  failed_checks.extend(result["failed_checks"])
                  elif "failed_checks" in data:
                      failed_checks = data.get("failed_checks", [])
              elif isinstance(data, list):
                  for item in data:
                      if isinstance(item, dict):
                          if "results" in item and "failed_checks" in item["results"]:
                              failed_checks.extend(item["results"]["failed_checks"])
                          elif "failed_checks" in item:
                              failed_checks.extend(item["failed_checks"])
              return failed_checks
          
          def generate_summary_report(failed_checks):
              s3_checks = ['CKV_AWS_20', 'CKV2_AWS_6', 'CKV_AWS_21', 'CKV_AWS_18', 'CKV_AWS_19']
              s3_issues = [check for check in failed_checks if check.get('check_id') in s3_checks]
              
              report = f"""# ğŸ›¡ï¸ S3 Security Scan Summary
          
          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
          **Commit:** {os.environ.get('GITHUB_SHA', 'unknown')[:8]}
          **Branch:** {os.environ.get('GITHUB_REF_NAME', 'unknown')}
          
          ## ğŸ“Š Results
          
          - **Total Failed Checks:** {len(failed_checks)}
          - **S3 Security Issues:** {len(s3_issues)} ğŸš¨
          - **Status:** {'âŒ Issues Found' if s3_issues else 'âœ… No S3 Issues'}
          
          ## ğŸš¨ S3 Issues Found
          
          """
              
              if not s3_issues:
                  report += "âœ… **No S3 security issues detected!**\n"
              else:
                  for i, issue in enumerate(s3_issues, 1):
                      report += f"""### {i}. {issue.get('check_name', 'Unknown Check')}
          
          - **Check ID:** `{issue.get('check_id', 'N/A')}`
          - **File:** `{issue.get('file_path', 'N/A')}`
          - **Resource:** `{issue.get('resource', 'N/A')}`
          - **Lines:** {'-'.join(map(str, issue.get('file_line_range', ['N/A'])))}
          - **Description:** {issue.get('description', 'N/A')}
          
          """
              
              # Debug info
              report += f"""
          ## ğŸ” Debug Information
          
          - **All Check IDs Found:** {', '.join(set(check.get('check_id', 'N/A') for check in failed_checks))}
          - **S3 Check IDs:** {', '.join(s3_checks)}
          - **Files Scanned:** {len(set(check.get('file_path', 'N/A') for check in failed_checks))}
          
          ## ğŸ”§ Next Steps
          
          {'1. Review the generated fixes in the `fixed_terraform/` directory' if s3_issues else ''}
          {'2. Apply the security improvements to make S3 buckets private' if s3_issues else ''}
          {'3. Re-run the security scan to verify fixes' if s3_issues else ''}
          
          ---
          *Generated by S3 Security Gatekeeper*
          """
              
              return report
          
          def main():
              data = load_checkov_report()
              if not data:
                  print("No Checkov data found")
                  return
              
              failed_checks = parse_checkov_data(data)
              report = generate_summary_report(failed_checks)
              
              with open("checkov_reports/security_summary.md", "w") as f:
                  f.write(report)
              
              print(f"Generated summary report with {len(failed_checks)} total issues")
          
          if __name__ == "__main__":
              main()
          EOF
          
          python scripts/generate_reports.py

      - name: ğŸ”§ Generate S3 Fixes
        if: github.event.inputs.auto_fix != 'false'
        run: |
          cat > scripts/fix_s3_buckets.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          import re
          from pathlib import Path
          from datetime import datetime
          
          S3_PUBLIC_CHECKS = {
              "CKV_AWS_20": "S3 Bucket has an ACL defined which allows public access",
              "CKV2_AWS_6": "Ensure that S3 bucket has S3 Bucket Public Access Block enabled",
              "CKV_AWS_21": "Ensure S3 bucket has server-side encryption enabled",
              "CKV_AWS_18": "Ensure S3 bucket has access logging enabled",
              "CKV_AWS_19": "Ensure S3 bucket has MFA delete enabled"
          }
          
          def load_checkov_report():
              report_path = Path("checkov_reports/report.json")
              if not report_path.exists():
                  print("âŒ No Checkov report found")
                  return None
              try:
                  with open(report_path, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Error loading report: {e}")
                  return None
          
          def parse_checkov_data(data):
              failed_checks = []
              if isinstance(data, dict):
                  if "results" in data:
                      results = data["results"]
                      if isinstance(results, dict) and "failed_checks" in results:
                          failed_checks = results["failed_checks"]
                      elif isinstance(results, list):
                          for result in results:
                              if isinstance(result, dict) and "failed_checks" in result:
                                  failed_checks.extend(result["failed_checks"])
                  elif "failed_checks" in data:
                      failed_checks = data.get("failed_checks", [])
              elif isinstance(data, list):
                  for item in data:
                      if isinstance(item, dict):
                          if "results" in item and "failed_checks" in item["results"]:
                              failed_checks.extend(item["results"]["failed_checks"])
                          elif "failed_checks" in item:
                              failed_checks.extend(item["failed_checks"])
              return failed_checks
          
          def identify_s3_issues(failed_checks):
              return [check for check in failed_checks if check.get("check_id") in S3_PUBLIC_CHECKS]
          
          def extract_bucket_name(resource_name):
              """Extract bucket name from resource identifier"""
              if '.' in resource_name:
                  return resource_name.split('.')[-1]
              return resource_name.replace('aws_s3_bucket.', '').replace('_acl', '').replace('_bucket', '')
          
          def fix_terraform_content(content, resource_name, check_id):
              bucket_name = extract_bucket_name(resource_name)
              
              if check_id == "CKV_AWS_20":  # Fix public ACL
                  patterns = [
                      (r'acl\s*=\s*"public-read"', 'acl = "private"'),
                      (r'acl\s*=\s*"public-read-write"', 'acl = "private"'),
                  ]
                  for pattern, replacement in patterns:
                      content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
              
              elif check_id == "CKV2_AWS_6":  # Add public access block
                  pab_config = f'''
          
          # Added by S3 Security Gatekeeper
          resource "aws_s3_bucket_public_access_block" "{bucket_name}_pab" {{
            bucket = aws_s3_bucket.{bucket_name}.id
          
            block_public_acls       = true
            block_public_policy     = true
            ignore_public_acls      = true
            restrict_public_buckets = true
          }}'''
                  if f'{bucket_name}_pab' not in content and 'aws_s3_bucket_public_access_block' not in content:
                      content += pab_config
              
              elif check_id == "CKV_AWS_21":  # Add encryption
                  encrypt_config = f'''
          
          # Added by S3 Security Gatekeeper
          resource "aws_s3_bucket_server_side_encryption_configuration" "{bucket_name}_encryption" {{
            bucket = aws_s3_bucket.{bucket_name}.id
          
            rule {{
              apply_server_side_encryption_by_default {{
                sse_algorithm = "AES256"
              }}
            }}
          }}'''
                  if f'{bucket_name}_encryption' not in content:
                      content += encrypt_config
              
              elif check_id == "CKV_AWS_18":  # Add access logging
                  logging_config = f'''
          
          # Added by S3 Security Gatekeeper
          resource "aws_s3_bucket" "{bucket_name}_logs" {{
            bucket = "{bucket_name}-access-logs"
          }}
          
          resource "aws_s3_bucket_logging" "{bucket_name}_logging" {{
            bucket = aws_s3_bucket.{bucket_name}.id
          
            target_bucket = aws_s3_bucket.{bucket_name}_logs.id
            target_prefix = "access-logs/"
          }}'''
                  if f'{bucket_name}_logging' not in content:
                      content += logging_config
              
              elif check_id == "CKV_AWS_19":  # Add versioning
                  versioning_config = f'''
          
          # Added by S3 Security Gatekeeper
          resource "aws_s3_bucket_versioning" "{bucket_name}_versioning" {{
            bucket = aws_s3_bucket.{bucket_name}.id
            versioning_configuration {{
              status = "Enabled"
            }}
          }}'''
                  if f'{bucket_name}_versioning' not in content:
                      content += versioning_config
              
              return content
          
          def main():
              print("ğŸ”§ Starting S3 fixes generation...")
              
              data = load_checkov_report()
              if not data:
                  print("No Checkov data found")
                  return
              
              failed_checks = parse_checkov_data(data)
              s3_issues = identify_s3_issues(failed_checks)
              
              print(f"ğŸ” Total failed checks: {len(failed_checks)}")
              print(f"ğŸ” S3 issues found: {len(s3_issues)}")
              
              if not s3_issues:
                  print("âœ… No S3 issues to fix")
                  return
              
              # Create fixed directory
              fixed_dir = Path("fixed_terraform")
              fixed_dir.mkdir(exist_ok=True)
              
              files_processed = set()
              
              for issue in s3_issues:
                  file_path = issue.get("file_path", "")
                  if not file_path or not os.path.exists(file_path):
                      print(f"âš ï¸ Skipping missing file: {file_path}")
                      continue
                  
                  if file_path in files_processed:
                      continue
                  
                  print(f"ğŸ”§ Processing {file_path}")
                  
                  try:
                      with open(file_path, 'r') as f:
                          content = f.read()
                      
                      original_content = content
                      
                      # Apply all fixes for this file
                      file_issues = [i for i in s3_issues if i.get("file_path") == file_path]
                      for file_issue in file_issues:
                          resource_name = file_issue.get("resource", "")
                          check_id = file_issue.get("check_id", "")
                          print(f"  Applying fix for {check_id}: {S3_PUBLIC_CHECKS.get(check_id, 'Unknown')}")
                          content = fix_terraform_content(content, resource_name, check_id)
                      
                      # Only write if content changed
                      if content != original_content:
                          output_path = fixed_dir / os.path.basename(file_path)
                          with open(output_path, 'w') as f:
                              f.write(content)
                          print(f"  âœ… Fixed version saved to: {output_path}")
                      else:
                          print(f"  âš ï¸ No changes made to {file_path}")
                      
                      files_processed.add(file_path)
                      
                  except Exception as e:
                      print(f"  âŒ Error processing {file_path}: {e}")
              
              # Generate summary
              summary_content = f"""# ğŸ”§ S3 Security Fixes Applied
          
          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
          **Files Processed:** {len(files_processed)}
          **Issues Fixed:** {len(s3_issues)}
          
          ## ğŸ“‹ Fixes Applied
          
          """
              
              for issue in s3_issues:
                  file_path = issue.get("file_path", "N/A")
                  check_id = issue.get("check_id", "N/A")
                  resource = issue.get("resource", "N/A")
                  description = S3_PUBLIC_CHECKS.get(check_id, "Unknown check")
                  
                  summary_content += f"""### {check_id}
          - **File:** `{file_path}`
          - **Resource:** `{resource}`
          - **Fix:** {description}
          
          """
              
              summary_content += """
          ## ğŸš€ Next Steps
          
          1. Review the fixed Terraform files in the `fixed_terraform/` directory
          2. Compare the changes with your original files
          3. Apply the security improvements to your infrastructure
          4. Re-run the security scan to verify all issues are resolved
          
          ## â— Important Notes
          
          - All fixes maintain backward compatibility where possible
          - Public ACLs have been changed to private
          - S3 bucket public access blocks have been enabled
          - Server-side encryption has been configured
          - Access logging and versioning have been added where missing
          
          ---
          *Generated by S3 Security Gatekeeper*
          """
              
              with open(fixed_dir / "FIXES_SUMMARY.md", "w") as f:
                  f.write(summary_content)
              
              print(f"âœ… Generated fixes for {len(s3_issues)} S3 security issues")
              print(f"ğŸ“ Fixed files available in: {fixed_dir}")
          
          if __name__ == "__main__":
              main()
          EOF
          
          python scripts/fix_s3_buckets.py

      - name: ğŸ“¤ Upload Fixed Files
        uses: actions/upload-artifact@v4
        with:
          name: s3-security-fixes
          path: |
            fixed_terraform/
            checkov_reports/security_summary.md
          retention-days: 30

      - name: ğŸ’¬ Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            try {
              // Read the security summary
              const summaryPath = 'checkov_reports/security_summary.md';
              let comment = '';
              
              if (fs.existsSync(summaryPath)) {
                comment = fs.readFileSync(summaryPath, 'utf8');
              } else {
                comment = `# ğŸ›¡ï¸ S3 Security Scan Results
                
                âŒ Security issues detected in S3 configurations!
                
                Please check the workflow logs and artifacts for detailed information.
                `;
              }
              
              // Add download links
              comment += `
              
              ## ğŸ“ Artifacts Available
              
              - **Security Report:** Check the workflow artifacts for detailed scan results
              - **Fixed Files:** Download the \`s3-security-fixes\` artifact for corrected Terraform files
              - **Original Scan:** Download the \`checkov-scan-results\` artifact for raw Checkov output
              `;
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.error('Error posting comment:', error);
            }

  # Job 3: Security Status Check
  security-status:
    name: ğŸ¯ Security Status
    runs-on: ubuntu-latest
    needs: [security-scan, generate-fixes]
    if: always()
    permissions:
      contents: read
    
    steps:
      - name: ğŸ“Š Report Final Status
        run: |
          echo "ğŸ” Security Scan Status: ${{ needs.security-scan.outputs.scan-status }}"
          echo "ğŸš¨ S3 Issues Found: ${{ needs.security-scan.outputs.s3-issues-count }}"
          echo "ğŸ›¡ï¸ Has S3 Issues: ${{ needs.security-scan.outputs.has-s3-issues }}"
          
          if [[ "${{ needs.security-scan.outputs.has-s3-issues }}" == "true" ]]; then
            echo "âŒ S3 security issues detected!"
            echo "ğŸ“ Check the artifacts for:"
            echo "  - Detailed security report"
            echo "  - Auto-generated fixes"
            echo "  - Original scan results"
            exit 1
          else
            echo "âœ… No S3 security issues found!"
          fi