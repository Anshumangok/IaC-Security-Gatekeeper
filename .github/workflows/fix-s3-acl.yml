name: ğŸ›¡ï¸ S3 Security Gatekeeper (Debug)

on:
  push:
    branches: [ main, develop ]
    paths:
      - '**/*.tf'
      - '**/*.json'
      - '.github/workflows/s3-security.yml'
  pull_request:
    branches: [ main ]
    paths:
      - '**/*.tf'
      - '**/*.json'
  workflow_dispatch:
    inputs:
      scan_all:
        description: 'Scan all files (not just changed)'
        required: false
        default: 'false'
        type: boolean
      auto_fix:
        description: 'Auto-generate fixes for S3 issues'
        required: false
        default: 'true'
        type: boolean
      debug_mode:
        description: 'Enable debug output'
        required: false
        default: 'true'
        type: boolean

env:
  CHECKOV_VERSION: "3.2.0"
  PYTHON_VERSION: "3.11"
  DEBUG: ${{ github.event.inputs.debug_mode || 'true' }}

jobs:
  # Job 1: Run Checkov Security Scan
  security-scan:
    name: ğŸ” Security Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
      pull-requests: write
    
    outputs:
      has-s3-issues: ${{ steps.check-s3-issues.outputs.has-issues }}
      scan-status: ${{ steps.checkov-scan.outputs.status }}
      s3-issues-count: ${{ steps.check-s3-issues.outputs.issues-count }}
      scan-completed: ${{ steps.checkov-scan.outputs.completed }}
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install Checkov
        run: |
          pip install checkov==${{ env.CHECKOV_VERSION }}
          checkov --version

      - name: ğŸ“ Create Reports Directory
        run: |
          mkdir -p checkov_reports scripts
          echo "Created directories:"
          ls -la checkov_reports/ scripts/

      - name: ğŸ” Debug - List Terraform Files
        if: env.DEBUG == 'true'
        run: |
          echo "=== Terraform files found ==="
          find . -name "*.tf" -type f | head -20
          echo "=== File count ==="
          find . -name "*.tf" -type f | wc -l

      - name: ğŸ” Run Checkov Scan
        id: checkov-scan
        continue-on-error: true
        run: |
          echo "ğŸ” Running Checkov security scan..."
          
          # Ensure reports directory exists
          mkdir -p checkov_reports
          
          # Check if we have any Terraform files
          TF_FILES=$(find . -name "*.tf" -type f | wc -l)
          echo "Found $TF_FILES Terraform files"
          
          if [ "$TF_FILES" -eq 0 ]; then
            echo "âš ï¸ No Terraform files found - creating empty report"
            echo '{"results": {"failed_checks": [], "passed_checks": []}}' > checkov_reports/report.json
            echo "status=no_tf_files" >> $GITHUB_OUTPUT
            echo "completed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Run Checkov with JSON output
          CHECKOV_EXIT_CODE=0
          checkov \
            --framework terraform \
            --output json \
            --output-file checkov_reports/report.json \
            --directory . \
            --skip-check CKV_TF_1 \
            --verbose || CHECKOV_EXIT_CODE=$?
          
          echo "checkov_exit_code=$CHECKOV_EXIT_CODE" >> $GITHUB_OUTPUT
          
          # Verify the report was created
          if [ ! -f "checkov_reports/report.json" ]; then
            echo "âŒ Checkov report not created - generating empty report"
            echo '{"results": {"failed_checks": [], "passed_checks": []}}' > checkov_reports/report.json
          fi
          
          # Also generate CLI output for logs
          echo "=== Checkov CLI Output ==="
          checkov \
            --framework terraform \
            --directory . \
            --compact || true
          
          echo "status=completed" >> $GITHUB_OUTPUT
          echo "completed=true" >> $GITHUB_OUTPUT

      - name: ğŸ” Debug - Show Checkov Report
        if: env.DEBUG == 'true'
        run: |
          echo "=== Directory Contents ==="
          ls -la checkov_reports/ || echo "checkov_reports directory not found"
          
          echo "=== Checkov Report Contents ==="
          if [ -f "checkov_reports/report.json" ]; then
            echo "Report file exists, size: $(wc -c < checkov_reports/report.json) bytes"
            echo "First 2000 characters:"
            head -c 2000 checkov_reports/report.json
            echo -e "\n=== Report structure ==="
            python3 -c "
          import json
          import sys
          try:
              with open('checkov_reports/report.json', 'r') as f:
                  data = json.load(f)
              print('Report type:', type(data))
              if isinstance(data, dict):
                  print('Keys:', list(data.keys()))
                  if 'results' in data:
                      results = data['results']
                      print('Results type:', type(results))
                      if isinstance(results, dict):
                          print('Results keys:', list(results.keys()))
                          if 'failed_checks' in results:
                              print('Failed checks count:', len(results['failed_checks']))
                          if 'passed_checks' in results:
                              print('Passed checks count:', len(results['passed_checks']))
                  elif 'failed_checks' in data:
                      print('Failed checks count:', len(data['failed_checks']))
              elif isinstance(data, list):
                  print('List length:', len(data))
                  if len(data) > 0:
                      print('First item keys:', list(data[0].keys()) if isinstance(data[0], dict) else 'Not a dict')
          except Exception as e:
              print('Error parsing report:', e)
              import traceback
              traceback.print_exc()
          "
          else
            echo "âŒ Report file not found!"
            echo "Files in current directory:"
            find . -name "*.json" -o -name "report*" | head -10
          fi

      - name: ğŸ“Š Check for S3 Issues
        id: check-s3-issues
        run: |
          cat > scripts/check_s3_issues.py << 'EOF'
          import json
          import sys
          from pathlib import Path
          
          def main():
              report_path = Path("checkov_reports/report.json")
              if not report_path.exists():
                  print("âŒ No Checkov report found", file=sys.stderr)
                  print("has-issues=false")
                  print("issues-count=0")
                  return
              
              try:
                  with open(report_path, 'r') as f:
                      data = json.load(f)
                  
                  failed_checks = []
                  
                  # Handle different Checkov report formats
                  if isinstance(data, dict):
                      if 'results' in data:
                          results = data['results']
                          if isinstance(results, dict) and 'failed_checks' in results:
                              failed_checks = results['failed_checks']
                          elif isinstance(results, list):
                              for result in results:
                                  if isinstance(result, dict) and 'failed_checks' in result:
                                      failed_checks.extend(result['failed_checks'])
                      elif 'failed_checks' in data:
                          failed_checks = data['failed_checks']
                  elif isinstance(data, list):
                      for item in data:
                          if isinstance(item, dict):
                              if 'results' in item and isinstance(item['results'], dict) and 'failed_checks' in item['results']:
                                  failed_checks.extend(item['results']['failed_checks'])
                              elif 'failed_checks' in item:
                                  failed_checks.extend(item['failed_checks'])
                  
                  print(f"Total failed checks found: {len(failed_checks)}", file=sys.stderr)
                  
                  # S3 security checks we're targeting
                  s3_checks = [
                      'CKV_AWS_20',  # S3 Bucket has an ACL defined which allows public access
                      'CKV2_AWS_6',  # S3 bucket has S3 Bucket Public Access Block enabled
                      'CKV_AWS_21',  # S3 bucket has server-side encryption enabled
                      'CKV_AWS_18',  # S3 bucket has access logging enabled
                      'CKV_AWS_19',  # S3 bucket has MFA delete enabled
                      'CKV_AWS_54',  # S3 bucket has block public policy enabled
                      'CKV_AWS_55',  # S3 bucket has ignore public ACLs enabled
                      'CKV_AWS_56'   # S3 bucket has restrict public bucket policies enabled
                  ]
                  
                  s3_issues = []
                  for check in failed_checks:
                      check_id = check.get('check_id', '')
                      if check_id in s3_checks:
                          s3_issues.append(check)
                          print(f"Found S3 issue: {check_id} in {check.get('file_path', 'unknown')}", file=sys.stderr)
                  
                  print(f"S3 issues found: {len(s3_issues)}", file=sys.stderr)
                  
                  # Force has-issues to true for testing if debug mode is on
                  if len(s3_issues) > 0:
                      print("has-issues=true")
                  else:
                      # For debugging - you can uncomment this to force generation
                      # print("has-issues=true")  # Force true for testing
                      print("has-issues=false")
                  
                  print(f"issues-count={len(s3_issues)}")
                  
                  # Debug: show all check IDs found
                  all_check_ids = sorted(set(check.get('check_id', '') for check in failed_checks))
                  print(f"All check IDs found: {all_check_ids}", file=sys.stderr)
                  
                  # Show sample of failed checks for debugging
                  if failed_checks:
                      print("Sample failed checks:", file=sys.stderr)
                      for i, check in enumerate(failed_checks[:3]):
                          print(f"  {i+1}. {check.get('check_id', 'N/A')} - {check.get('check_name', 'N/A')}", file=sys.stderr)
                  
              except Exception as e:
                  print(f"Error processing report: {e}", file=sys.stderr)
                  import traceback
                  traceback.print_exc(file=sys.stderr)
                  print("has-issues=false")
                  print("issues-count=0")
          
          if __name__ == "__main__":
              main()
          EOF
          
          # Run the check and capture outputs
          python scripts/check_s3_issues.py >> $GITHUB_OUTPUT
          echo "ğŸ” S3 Issues Check completed"

      - name: ğŸ“¤ Upload Scan Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: checkov-scan-results
          path: |
            checkov_reports/
            scripts/
          retention-days: 30
          if-no-files-found: warn

  # Job 2: Generate Reports and Fixes
  generate-fixes:
    name: ğŸ”§ Generate S3 Fixes
    runs-on: ubuntu-latest
    needs: security-scan
    # Modified condition to always run if scan completed successfully
    if: always() && needs.security-scan.outputs.scan-completed == 'true'
    permissions:
      contents: write
      pull-requests: write
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“ Create Scripts Directory
        run: mkdir -p scripts fixed_terraform

      - name: ğŸ“¥ Download Scan Results
        uses: actions/download-artifact@v4
        with:
          name: checkov-scan-results
          path: ./

      - name: ğŸ” Debug - Verify Downloaded Files
        if: env.DEBUG == 'true'
        run: |
          echo "=== Downloaded files ==="
          find . -name "*.json" -o -name "*.py" -o -name "*.md" | head -20
          if [ -f "checkov_reports/report.json" ]; then
            echo "Report size: $(wc -c < checkov_reports/report.json) bytes"
          fi

      - name: ğŸ“ Generate Markdown Report
        run: |
          cat > scripts/generate_reports.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          
          def load_checkov_report():
              report_path = Path("checkov_reports/report.json")
              if not report_path.exists():
                  print("âŒ No Checkov report found")
                  return None
              try:
                  with open(report_path, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Error loading report: {e}")
                  return None
          
          def parse_checkov_data(data):
              failed_checks = []
              if isinstance(data, dict):
                  if "results" in data:
                      results = data["results"]
                      if isinstance(results, dict) and "failed_checks" in results:
                          failed_checks = results["failed_checks"]
                      elif isinstance(results, list):
                          for result in results:
                              if isinstance(result, dict) and "failed_checks" in result:
                                  failed_checks.extend(result["failed_checks"])
                  elif "failed_checks" in data:
                      failed_checks = data.get("failed_checks", [])
              elif isinstance(data, list):
                  for item in data:
                      if isinstance(item, dict):
                          if "results" in item and isinstance(item["results"], dict) and "failed_checks" in item["results"]:
                              failed_checks.extend(item["results"]["failed_checks"])
                          elif "failed_checks" in item:
                              failed_checks.extend(item["failed_checks"])
              return failed_checks
          
          def generate_summary_report(failed_checks):
              s3_checks = ['CKV_AWS_20', 'CKV2_AWS_6', 'CKV_AWS_21', 'CKV_AWS_18', 'CKV_AWS_19', 'CKV_AWS_54', 'CKV_AWS_55', 'CKV_AWS_56']
              s3_issues = [check for check in failed_checks if check.get('check_id') in s3_checks]
              
              report = f"""# ğŸ›¡ï¸ S3 Security Scan Summary
          
          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
          **Commit:** {os.environ.get('GITHUB_SHA', 'unknown')[:8]}
          **Branch:** {os.environ.get('GITHUB_REF_NAME', 'unknown')}
          
          ## ğŸ“Š Results
          
          - **Total Failed Checks:** {len(failed_checks)}
          - **S3 Security Issues:** {len(s3_issues)} {'ğŸš¨' if s3_issues else 'âœ…'}
          - **Status:** {'âŒ Issues Found' if s3_issues else 'âœ… No S3 Issues'}
          
          ## ğŸš¨ S3 Issues Found
          
          """
              
              if not s3_issues:
                  report += "âœ… **No S3 security issues detected!**\n\n"
                  if failed_checks:
                      report += f"However, {len(failed_checks)} other security issues were found.\n\n"
              else:
                  for i, issue in enumerate(s3_issues, 1):
                      report += f"""### {i}. {issue.get('check_name', 'Unknown Check')}
          
          - **Check ID:** `{issue.get('check_id', 'N/A')}`
          - **File:** `{issue.get('file_path', 'N/A')}`
          - **Resource:** `{issue.get('resource', 'N/A')}`
          - **Lines:** {'-'.join(map(str, issue.get('file_line_range', ['N/A'])))}
          - **Description:** {issue.get('description', 'N/A')}
          
          """
              
              # Debug info
              all_check_ids = sorted(set(check.get('check_id', 'N/A') for check in failed_checks))
              report += f"""
          ## ğŸ” Debug Information
          
          - **All Check IDs Found:** {', '.join(all_check_ids) if all_check_ids else 'None'}
          - **S3 Check IDs Monitored:** {', '.join(s3_checks)}
          - **Files Scanned:** {len(set(check.get('file_path', 'N/A') for check in failed_checks))}
          - **Total Issues:** {len(failed_checks)}
          
          ## ğŸ”§ Next Steps
          
          {'1. Review the generated fixes in the `fixed_terraform/` directory' if s3_issues else ''}
          {'2. Apply the security improvements to make S3 buckets private' if s3_issues else ''}
          {'3. Re-run the security scan to verify fixes' if s3_issues else ''}
          {'' if s3_issues else '1. Review other security issues found in the scan'}
          {'' if s3_issues else '2. Check the Checkov report for details on non-S3 issues'}
          
          ---
          *Generated by S3 Security Gatekeeper*
          """
              
              return report
          
          def main():
              data = load_checkov_report()
              if not data:
                  print("No Checkov data found")
                  return
              
              failed_checks = parse_checkov_data(data)
              report = generate_summary_report(failed_checks)
              
              # Ensure directory exists
              os.makedirs("checkov_reports", exist_ok=True)
              
              with open("checkov_reports/security_summary.md", "w") as f:
                  f.write(report)
              
              print(f"Generated summary report with {len(failed_checks)} total issues")
          
          if __name__ == "__main__":
              main()
          EOF
          
          python scripts/generate_reports.py

      - name: ğŸ”§ Generate S3 Fixes
        run: |
          cat > scripts/fix_s3_buckets.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          import re
          from pathlib import Path
          from datetime import datetime
          
          S3_PUBLIC_CHECKS = {
              "CKV_AWS_20": "S3 Bucket has an ACL defined which allows public access",
              "CKV2_AWS_6": "Ensure that S3 bucket has S3 Bucket Public Access Block enabled",
              "CKV_AWS_21": "Ensure S3 bucket has server-side encryption enabled",
              "CKV_AWS_18": "Ensure S3 bucket has access logging enabled",
              "CKV_AWS_19": "Ensure S3 bucket has MFA delete enabled",
              "CKV_AWS_54": "Ensure S3 bucket has block public policy enabled",
              "CKV_AWS_55": "Ensure S3 bucket has ignore public ACLs enabled",
              "CKV_AWS_56": "Ensure S3 bucket has restrict public bucket policies enabled"
          }
          
          def load_checkov_report():
              report_path = Path("checkov_reports/report.json")
              if not report_path.exists():
                  print("âŒ No Checkov report found")
                  return None
              try:
                  with open(report_path, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Error loading report: {e}")
                  return None
          
          def parse_checkov_data(data):
              failed_checks = []
              if isinstance(data, dict):
                  if "results" in data:
                      results = data["results"]
                      if isinstance(results, dict) and "failed_checks" in results:
                          failed_checks = results["failed_checks"]
                      elif isinstance(results, list):
                          for result in results:
                              if isinstance(result, dict) and "failed_checks" in result:
                                  failed_checks.extend(result["failed_checks"])
                  elif "failed_checks" in data:
                      failed_checks = data.get("failed_checks", [])
              elif isinstance(data, list):
                  for item in data:
                      if isinstance(item, dict):
                          if "results" in item and isinstance(item["results"], dict) and "failed_checks" in item["results"]:
                              failed_checks.extend(item["results"]["failed_checks"])
                          elif "failed_checks" in item:
                              failed_checks.extend(item["failed_checks"])
              return failed_checks
          
          def identify_s3_issues(failed_checks):
              return [check for check in failed_checks if check.get("check_id") in S3_PUBLIC_CHECKS]
          
          def extract_bucket_name(resource_name):
              """Extract bucket name from resource identifier"""
              if '.' in resource_name:
                  return resource_name.split('.')[-1]
              return resource_name.replace('aws_s3_bucket.', '').replace('_acl', '').replace('_bucket', '')
          
          def fix_terraform_content(content, resource_name, check_id):
              try:
                  bucket_name = extract_bucket_name(resource_name)
                  
                  if check_id == "CKV_AWS_20":  # Fix public ACL
                      patterns = [
                          (r'acl\s*=\s*"public-read"', 'acl = "private"'),
                          (r'acl\s*=\s*"public-read-write"', 'acl = "private"'),
                      ]
                      for pattern, replacement in patterns:
                          content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
                  
                  elif check_id in ["CKV2_AWS_6", "CKV_AWS_54", "CKV_AWS_55", "CKV_AWS_56"]:  # Add public access block
                      pab_config = f'''

# Added by S3 Security Gatekeeper
resource "aws_s3_bucket_public_access_block" "{bucket_name}_pab" {{
  bucket = aws_s3_bucket.{bucket_name}.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}}'''
    if f'{bucket_name}_pab' not in content and 'aws_s3_bucket_public_access_block' not in content:
        content += pab_config

elif check_id == "CKV_AWS_21":  # Add encryption
    encrypt_config = f'''

# Added by S3 Security Gatekeeper
resource "aws_s3_bucket_server_side_encryption_configuration" "{bucket_name}_encryption" {{
  bucket = aws_s3_bucket.{bucket_name}.id

  rule {{
    apply_server_side_encryption_by_default {{
      sse_algorithm = "AES256"
    }}
  }}
}}'''
    if f'{bucket_name}_encryption' not in content:
        content += encrypt_config

elif check_id == "CKV_AWS_18":  # Add access logging
    logging_config = f'''

# Added by S3 Security Gatekeeper
resource "aws_s3_bucket" "{bucket_name}_logs" {{
  bucket = "{bucket_name}-access-logs"
}}

resource "aws_s3_bucket_logging" "{bucket_name}_logging" {{
  bucket = aws_s3_bucket.{bucket_name}.id

  target_bucket = aws_s3_bucket.{bucket_name}_logs.id
  target_prefix = "access-logs/"
}}'''
    if f'{bucket_name}_logging' not in content:
        content += logging_config

elif check_id == "CKV_AWS_19":  # Add versioning
    versioning_config = f'''

# Added by S3 Security Gatekeeper
resource "aws_s3_bucket_versioning" "{bucket_name}_versioning" {{
  bucket = aws_s3_bucket.{bucket_name}.id
  versioning_configuration {{
    status = "Enabled"
  }}
}}'''   
                      if f'{bucket_name}_versioning' not in content:
                          content += versioning_config
                  
                  return content
              except Exception as e:
                  print(f"Error in fix_terraform_content: {e}")
                  return content
          
          def main():
              print("ğŸ”§ Starting S3 fixes generation...")
              
              data = load_checkov_report()
              if not data:
                  print("No Checkov data found")
                  return
              
              failed_checks = parse_checkov_data(data)
              s3_issues = identify_s3_issues(failed_checks)
              
              print(f"ğŸ” Total failed checks: {len(failed_checks)}")
              print(f"ğŸ” S3 issues found: {len(s3_issues)}")
              
              # Create fixed directory
              fixed_dir = Path("fixed_terraform")
              fixed_dir.mkdir(exist_ok=True)
              
              if not s3_issues:
                  print("âœ… No S3 issues to fix")
                  # Still create a summary file
                  summary_content = f"""# ğŸ”§ S3 Security Fixes Summary

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
**Status:** âœ… No S3 issues found to fix

## ğŸ“Š Scan Results
- **Total Issues:** {len(failed_checks)}
- **S3 Issues:** 0

---
*Generated by S3 Security Gatekeeper*
"""
                  
                  with open(fixed_dir / "FIXES_SUMMARY.md", "w") as f:
                      f.write(summary_content)
                  
                  return
              
              files_processed = set()
              
              for issue in s3_issues:
                  file_path = issue.get("file_path", "")
                  if not file_path or not os.path.exists(file_path):
                      print(f"âš ï¸ Skipping missing file: {file_path}")
                      continue
                  
                  if file_path in files_processed:
                      continue
                  
                  print(f"ğŸ”§ Processing {file_path}")
                  
                  try:
                      with open(file_path, 'r', encoding='utf-8') as f:
                          content = f.read()
                      
                      original_content = content
                      
                      # Apply all fixes for this file
                      file_issues = [i for i in s3_issues if i.get("file_path") == file_path]
                      for file_issue in file_issues:
                          resource_name = file_issue.get("resource", "")
                          check_id = file_issue.get("check_id", "")
                          print(f"  Applying fix for {check_id}: {S3_PUBLIC_CHECKS.get(check_id, 'Unknown')}")
                          content = fix_terraform_content(content, resource_name, check_id)
                      
                      # Only write if content changed
                      if content != original_content:
                          output_path = fixed_dir / os.path.basename(file_path)
                          with open(output_path, 'w', encoding='utf-8') as f:
                              f.write(content)
                          print(f"  âœ… Fixed version saved to: {output_path}")
                      else:
                          print(f"  âš ï¸ No changes made to {file_path}")
                      
                      files_processed.add(file_path)
                      
                  except Exception as e:
                      print(f"  âŒ Error processing {file_path}: {e}")
                      import traceback
                      traceback.print_exc()
              
              # Generate summary
              summary_content = f"""# ğŸ”§ S3 Security Fixes Applied

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
**Files Processed:** {len(files_processed)}
**Issues Fixed:** {len(s3_issues)}

## ğŸ“‹ Fixes Applied

"""
              
              for issue in s3_issues:
                  file_path = issue.get("file_path", "N/A")
                  check_id = issue.get("check_id", "N/A")
                  resource = issue.get("resource", "N/A")
                  description = S3_PUBLIC_CHECKS.get(check_id, "Unknown check")
                  
                  summary_content += f"""### {check_id}
- **File:** `{file_path}`
- **Resource:** `{resource}`
- **Fix:** {description}

"""
              
              summary_content += """
## ğŸš€ Next Steps

1. Review the fixed Terraform files in the `fixed_terraform/` directory
2. Compare the changes with your original files
3. Apply the security improvements to your infrastructure
4. Re-run the security scan to verify fixes

---
*Generated by S3 Security Gatekeeper*
"""
              
              with open(fixed_dir / "FIXES_SUMMARY.md", "w") as f:
                  f.write(summary_content)
              
              print(f"âœ… Generated fixes for {len(s3_issues)} S3 issues across {len(files_processed)} files")
          
          if __name__ == "__main__":
              main()
          EOF
          
          python scripts/fix_s3_buckets.py

      - name: ğŸ“ Create Fix Instructions
        run: |
          cat > scripts/create_instructions.py << 'EOF'
          #!/usr/bin/env python3
          import os
          from pathlib import Path
          from datetime import datetime
          
          def create_instructions():
              fixed_dir = Path("fixed_terraform")
              
              instructions = f"""# ğŸ”§ How to Apply S3 Security Fixes
          
          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
          **Repository:** {os.environ.get('GITHUB_REPOSITORY', 'Unknown')}
          **Commit:** {os.environ.get('GITHUB_SHA', 'Unknown')[:8]}
          
          ## ğŸ“ Files Generated
          
          This workflow has generated secure versions of your Terraform files with S3 security improvements.
          
          ### Directory Structure
          ```
          fixed_terraform/
          â”œâ”€â”€ FIXES_SUMMARY.md         # Summary of all fixes applied
          â”œâ”€â”€ INSTRUCTIONS.md          # This file
          â””â”€â”€ *.tf                     # Your fixed Terraform files
          ```
          
          ## ğŸš€ Quick Start
          
          1. **Download the artifacts** from this GitHub Actions run
          2. **Review the changes** in the `fixed_terraform/` directory
          3. **Replace your original files** with the fixed versions
          4. **Test and apply** your Terraform configuration
          
          ## ğŸ” What Was Fixed
          
          The following S3 security issues were automatically resolved:
          
          - **CKV_AWS_20**: Removed public ACL configurations
          - **CKV2_AWS_6**: Added S3 bucket public access block
          - **CKV_AWS_21**: Added server-side encryption
          - **CKV_AWS_18**: Added access logging configuration
          - **CKV_AWS_19**: Added versioning configuration
          - **CKV_AWS_54-56**: Enhanced public access restrictions
          
          ## âš ï¸ Important Notes
          
          - **Review before applying**: Always review the generated fixes before applying them to production
          - **Test thoroughly**: Test the changes in a development environment first
          - **Backup original**: Keep backups of your original Terraform files
          - **Cost implications**: Some fixes (like logging) may incur additional AWS costs
          
          ## ğŸ› ï¸ Manual Steps Required
          
          Some security improvements may require manual configuration:
          
          1. **MFA Delete**: Enable MFA delete in the AWS console for critical buckets
          2. **Bucket Policies**: Review and update bucket policies if needed
          3. **IAM Permissions**: Ensure your IAM roles have the necessary permissions
          
          ## ğŸ”„ Re-running the Scan
          
          After applying the fixes, you can re-run the security scan by:
          
          1. Pushing your changes to the repository
          2. Or manually triggering the workflow with the "Run workflow" button
          
          ## ğŸ“ Support
          
          If you encounter issues with the generated fixes:
          
          1. Check the workflow logs for detailed error messages
          2. Review the `FIXES_SUMMARY.md` file for specific changes made
          3. Consult the Checkov documentation for specific check details
          
          ---
          *Generated by S3 Security Gatekeeper v1.0*
          """
              
              fixed_dir.mkdir(exist_ok=True)
              with open(fixed_dir / "INSTRUCTIONS.md", "w") as f:
                  f.write(instructions)
              
              print("ğŸ“ Created application instructions")
          
          if __name__ == "__main__":
              create_instructions()
          EOF
          
          python scripts/create_instructions.py

      - name: ğŸ“¤ Upload Fixed Files
        uses: actions/upload-artifact@v4
        with:
          name: s3-security-fixes
          path: |
            fixed_terraform/
            checkov_reports/
          retention-days: 30
          if-no-files-found: warn

      - name: ğŸ“Š Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Read the summary report
            let summaryContent = '';
            try {
              summaryContent = fs.readFileSync('checkov_reports/security_summary.md', 'utf8');
            } catch (error) {
              summaryContent = 'âŒ Could not read security summary report';
            }
            
            // Check if fixes were generated
            const fixedDir = 'fixed_terraform';
            let fixesGenerated = false;
            try {
              const files = fs.readdirSync(fixedDir);
              fixesGenerated = files.some(file => file.endsWith('.tf'));
            } catch (error) {
              // Directory doesn't exist or is empty
            }
            
            const comment = `## ğŸ›¡ï¸ S3 Security Scan Results
            
            ${summaryContent}
            
            ${fixesGenerated ? 
              `## ğŸ”§ Automated Fixes Available
              
              âœ… Security fixes have been generated and are available in the workflow artifacts.
              
              **To apply the fixes:**
              1. Download the \`s3-security-fixes\` artifact from this workflow run
              2. Review the changes in the \`fixed_terraform/\` directory
              3. Apply the fixes to your repository
              
              ğŸ“‹ See \`INSTRUCTIONS.md\` in the artifact for detailed steps.` :
              `## âœ… No S3 Issues Found
              
              No S3-specific security issues were detected that require fixes.`
            }
            
            ---
            *ğŸ¤– Comment generated by S3 Security Gatekeeper*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Job 3: Security Summary
  security-summary:
    name: ğŸ“‹ Security Summary
    runs-on: ubuntu-latest
    needs: [security-scan, generate-fixes]
    if: always()
    permissions:
      contents: read
      security-events: write
    
    steps:
      - name: ğŸ“¥ Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          merge-multiple: true

      - name: ğŸ“Š Generate Final Summary
        run: |
          echo "# ğŸ›¡ï¸ S3 Security Gatekeeper - Final Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** $GITHUB_REPOSITORY" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** $GITHUB_REF_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${GITHUB_SHA:0:8}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Scan Results
          echo "## ğŸ” Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Scan Status:** ${{ needs.security-scan.outputs.scan-status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **S3 Issues Found:** ${{ needs.security-scan.outputs.s3-issues-count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Has S3 Issues:** ${{ needs.security-scan.outputs.has-s3-issues }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Job Status
          echo "## ğŸ¯ Job Status" >> $GITHUB_STEP_SUMMARY
          echo "- **Security Scan:** ${{ needs.security-scan.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Generate Fixes:** ${{ needs.generate-fixes.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Next Steps
          if [ "${{ needs.security-scan.outputs.has-s3-issues }}" = "true" ]; then
            echo "## ğŸ”§ Next Steps" >> $GITHUB_STEP_SUMMARY
            echo "1. ğŸ“¥ Download the \`s3-security-fixes\` artifact" >> $GITHUB_STEP_SUMMARY
            echo "2. ğŸ“‹ Review the generated fixes and instructions" >> $GITHUB_STEP_SUMMARY
            echo "3. ğŸš€ Apply the security improvements to your infrastructure" >> $GITHUB_STEP_SUMMARY
            echo "4. ğŸ”„ Re-run this workflow to verify fixes" >> $GITHUB_STEP_SUMMARY
          else
            echo "## âœ… All Clear!" >> $GITHUB_STEP_SUMMARY
            echo "No S3 security issues detected. Your S3 configurations appear secure." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Generated by S3 Security Gatekeeper*" >> $GITHUB_STEP_SUMMARY

      - name: ğŸ¯ Set Workflow Status
        run: |
          if [ "${{ needs.security-scan.outputs.has-s3-issues }}" = "true" ]; then
            echo "âš ï¸ S3 security issues detected - check artifacts for fixes"
            exit 0  # Don't fail the workflow, just notify
          else
            echo "âœ… No S3 security issues detected"
          fi

      - name: ğŸ“ˆ Upload SARIF (if available)
        uses: github/codeql-action/upload-sarif@v3
        if: always() && hashFiles('checkov_reports/*.sarif') != ''
        with:
          sarif_file: checkov_reports/
        continue-on-error: true