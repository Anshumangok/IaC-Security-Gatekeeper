name: ðŸ›¡ï¸ S3 Security Gatekeeper

on:
  push:
    branches: [ main, develop ]
    paths:
      - '**/*.tf'
      - '**/*.json'
      - '.github/workflows/s3-security.yml'
  pull_request:
    branches: [ main ]
    paths:
      - '**/*.tf'
      - '**/*.json'
  workflow_dispatch:
    inputs:
      scan_all:
        description: 'Scan all files (not just changed)'
        required: false
        default: 'false'
        type: boolean
      auto_fix:
        description: 'Auto-generate fixes for S3 issues'
        required: false
        default: 'true'
        type: boolean

env:
  CHECKOV_VERSION: "3.2.0"
  PYTHON_VERSION: "3.11"

jobs:
  # Job 1: Run Checkov Security Scan
  security-scan:
    name: ðŸ” Security Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
      pull-requests: write
    
    outputs:
      has-s3-issues: ${{ steps.check-s3-issues.outputs.has-issues }}
      scan-status: ${{ steps.checkov-scan.outputs.status }}
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ðŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ðŸ“¦ Install Checkov
        run: |
          pip install checkov==${{ env.CHECKOV_VERSION }}
          checkov --version

      - name: ðŸ“ Create Reports Directory
        run: mkdir -p checkov_reports scripts

      - name: ðŸ” Run Checkov Scan
        id: checkov-scan
        continue-on-error: true
        run: |
          echo "ðŸ” Running Checkov security scan..."
          
          # Run Checkov with JSON output
          checkov \
            --framework terraform \
            --output json \
            --output-file checkov_reports/report.json \
            --quiet \
            --compact \
            --directory . \
            --skip-check CKV_TF_1 \
            || echo "status=failed" >> $GITHUB_OUTPUT
          
          # Also generate CLI output for logs
          checkov \
            --framework terraform \
            --directory . \
            --quiet \
            --compact || true
          
          echo "status=completed" >> $GITHUB_OUTPUT

      - name: ðŸ“Š Check for S3 Issues
        id: check-s3-issues
        run: |
          if [ -f "checkov_reports/report.json" ]; then
            # Check if S3-related issues exist
            S3_ISSUES=$(python3 -c "
          import json
          import sys
          
          try:
              with open('checkov_reports/report.json', 'r') as f:
                  data = json.load(f)
              
              failed_checks = []
              if isinstance(data, dict):
                  if 'results' in data:
                      failed_checks = data['results'].get('failed_checks', [])
                  elif 'failed_checks' in data:
                      failed_checks = data.get('failed_checks', [])
              elif isinstance(data, list):
                  for item in data:
                      if isinstance(item, dict) and 'results' in item:
                          failed_checks.extend(item['results'].get('failed_checks', []))
              
              s3_checks = ['CKV_AWS_20', 'CKV2_AWS_6', 'CKV_AWS_21', 'CKV_AWS_18', 'CKV_AWS_19']
              s3_issues = [check for check in failed_checks if check.get('check_id') in s3_checks]
              
              print(f'Found {len(s3_issues)} S3 security issues')
              print('true' if s3_issues else 'false')
          except Exception as e:
              print(f'Error: {e}', file=sys.stderr)
              print('false')
          ")
            
            echo "has-issues=$S3_ISSUES" >> $GITHUB_OUTPUT
            echo "ðŸ” S3 Issues Check: $S3_ISSUES"
          else
            echo "has-issues=false" >> $GITHUB_OUTPUT
            echo "âŒ No Checkov report found"
          fi

      - name: ðŸ“¤ Upload Scan Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: checkov-scan-results
          path: checkov_reports/
          retention-days: 30

  # Job 2: Generate Reports and Fixes
  generate-fixes:
    name: ðŸ”§ Generate S3 Fixes
    runs-on: ubuntu-latest
    needs: security-scan
    if: needs.security-scan.outputs.has-s3-issues == 'true'
    permissions:
      contents: write
      pull-requests: write
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ðŸ“ Create Scripts Directory
        run: mkdir -p scripts

      - name: ðŸ“¥ Download Scan Results
        uses: actions/download-artifact@v4
        with:
          name: checkov-scan-results
          path: checkov_reports/

      - name: ðŸ“ Generate Markdown Report
        run: |
          cat > scripts/generate_reports.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          
          def load_checkov_report():
              report_path = Path("checkov_reports/report.json")
              if not report_path.exists():
                  return None
              try:
                  with open(report_path, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Error loading report: {e}")
                  return None
          
          def parse_checkov_data(data):
              failed_checks = []
              if isinstance(data, dict):
                  if "results" in data:
                      failed_checks = data["results"].get("failed_checks", [])
                  elif "failed_checks" in data:
                      failed_checks = data.get("failed_checks", [])
              elif isinstance(data, list):
                  for item in data:
                      if isinstance(item, dict):
                          if "results" in item:
                              failed_checks.extend(item["results"].get("failed_checks", []))
              return failed_checks
          
          def generate_summary_report(failed_checks):
              s3_checks = ['CKV_AWS_20', 'CKV2_AWS_6', 'CKV_AWS_21', 'CKV_AWS_18', 'CKV_AWS_19']
              s3_issues = [check for check in failed_checks if check.get('check_id') in s3_checks]
              
              report = f"""# ðŸ›¡ï¸ S3 Security Scan Summary
          
          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
          **Commit:** {os.environ.get('GITHUB_SHA', 'unknown')[:8]}
          **Branch:** {os.environ.get('GITHUB_REF_NAME', 'unknown')}
          
          ## ðŸ“Š Results
          
          - **Total Failed Checks:** {len(failed_checks)}
          - **S3 Security Issues:** {len(s3_issues)} ðŸš¨
          - **Status:** {'âŒ Issues Found' if s3_issues else 'âœ… No S3 Issues'}
          
          ## ðŸš¨ S3 Issues Found
          
          """
              
              if not s3_issues:
                  report += "âœ… **No S3 security issues detected!**\n"
              else:
                  for i, issue in enumerate(s3_issues, 1):
                      report += f"""### {i}. {issue.get('check_name', 'Unknown Check')}
          
          - **Check ID:** `{issue.get('check_id', 'N/A')}`
          - **File:** `{issue.get('file_path', 'N/A')}`
          - **Resource:** `{issue.get('resource', 'N/A')}`
          - **Lines:** {'-'.join(map(str, issue.get('file_line_range', ['N/A'])))}
          
          """
              
              report += f"""
          ## ðŸ”§ Next Steps
          
          {'1. Review the generated fixes in the `fixed_terraform/` directory' if s3_issues else ''}
          {'2. Apply the security improvements to make S3 buckets private' if s3_issues else ''}
          {'3. Re-run the security scan to verify fixes' if s3_issues else ''}
          
          ---
          *Generated by S3 Security Gatekeeper*
          """
              
              return report
          
          def main():
              data = load_checkov_report()
              if not data:
                  print("No Checkov data found")
                  return
              
              failed_checks = parse_checkov_data(data)
              report = generate_summary_report(failed_checks)
              
              with open("checkov_reports/security_summary.md", "w") as f:
                  f.write(report)
              
              print(f"Generated summary report with {len(failed_checks)} total issues")
          
          if __name__ == "__main__":
              main()
          EOF
          
          python scripts/generate_reports.py

      - name: ðŸ”§ Generate S3 Fixes
        if: github.event.inputs.auto_fix != 'false'
        run: |
          # Copy the S3 fixer script
          cat > scripts/fix_s3_buckets.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          import re
          from pathlib import Path
          from datetime import datetime
          
          S3_PUBLIC_CHECKS = {
              "CKV_AWS_20": "S3 Bucket has an ACL defined which allows public access",
              "CKV2_AWS_6": "Ensure that S3 bucket has S3 Bucket Public Access Block enabled",
              "CKV_AWS_21": "Ensure S3 bucket has server-side encryption enabled",
              "CKV_AWS_18": "Ensure S3 bucket has access logging enabled",
              "CKV_AWS_19": "Ensure S3 bucket has MFA delete enabled"
          }
          
          def load_checkov_report():
              report_path = Path("checkov_reports/report.json")
              if not report_path.exists():
                  return None
              try:
                  with open(report_path, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Error loading report: {e}")
                  return None
          
          def parse_checkov_data(data):
              failed_checks = []
              if isinstance(data, dict):
                  if "results" in data:
                      failed_checks = data["results"].get("failed_checks", [])
                  elif "failed_checks" in data:
                      failed_checks = data.get("failed_checks", [])
              elif isinstance(data, list):
                  for item in data:
                      if isinstance(item, dict):
                          if "results" in item:
                              failed_checks.extend(item["results"].get("failed_checks", []))
              return failed_checks
          
          def identify_s3_issues(failed_checks):
              return [check for check in failed_checks if check.get("check_id") in S3_PUBLIC_CHECKS]
          
          def fix_terraform_content(content, resource_name, check_id):
              if check_id == "CKV_AWS_20":  # Fix public ACL
                  patterns = [
                      (r'acl\s*=\s*"public-read"', 'acl = "private"'),
                      (r'acl\s*=\s*"public-read-write"', 'acl = "private"'),
                  ]
                  for pattern, replacement in patterns:
                      content = re.sub(pattern, replacement, content)
              
              elif check_id == "CKV2_AWS_6":  # Add public access block
                  bucket_name = resource_name.split('.')[-1].replace('_acl', '').replace('_bucket', '')
                  pab_config = f'''
          resource "aws_s3_bucket_public_access_block" "{bucket_name}_pab" {{
            bucket = aws_s3_bucket.{bucket_name}.id
          
            block_public_acls       = true
            block_public_policy     = true
            ignore_public_acls      = true
            restrict_public_buckets = true
          }}
          '''
                  if 'aws_s3_bucket_public_access_block' not in content:
                      content += pab_config
              
              elif check_id == "CKV_AWS_21":  # Add encryption
                  bucket_name = resource_name.split('.')[-1].replace('_acl', '').replace('_bucket', '')
                  encrypt_config = f'''
          resource "aws_s3_bucket_server_side_encryption_configuration" "{bucket_name}_encryption" {{
            bucket = aws_s3_bucket.{bucket_name}.id
          
            rule {{
              apply_server_side_encryption_by_default {{
                sse_algorithm = "AES256"
              }}
            }}
          }}
          '''
                  if 'aws_s3_bucket_server_side_encryption_configuration' not in content:
                      content += encrypt_config
              
              elif check_id == "CKV_AWS_18":  # Add access logging
                  bucket_name = resource_name.split('.')[-1].replace('_acl', '').replace('_bucket', '')
                  logging_config = f'''
          resource "aws_s3_bucket_logging" "{bucket_name}_logging" {{
            bucket = aws_s3_bucket.{bucket_name}.id
          
            target_bucket = aws_s3_bucket.{bucket_name}_logs.id
            target_prefix = "log/"
          }}
          
          resource "aws_s3_bucket" "{bucket_name}_logs" {{
            bucket = "{bucket_name}-access-logs"
          }}
          '''
                  if 'aws_s3_bucket_logging' not in content:
                      content += logging_config
              
              elif check_id == "CKV_AWS_19":  # Add MFA delete (requires versioning)
                  bucket_name = resource_name.split('.')[-1].replace('_acl', '').replace('_bucket', '')
                  versioning_config = f'''
          resource "aws_s3_bucket_versioning" "{bucket_name}_versioning" {{
            bucket = aws_s3_bucket.{bucket_name}.id
            versioning_configuration {{
              status     = "Enabled"
              mfa_delete = "Enabled"
            }}
          }}
          '''
                  if 'aws_s3_bucket_versioning' not in content:
                      content += versioning_config
              
              return content
          
          def generate_fix_summary(s3_issues, files_processed):
              summary = f"""# ðŸ”§ S3 Security Fixes Applied
          
          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
          **Files Processed:** {len(files_processed)}
          **Issues Fixed:** {len(s3_issues)}
          
          ## ðŸ› ï¸ Fixes Applied
          
          """
              
              for issue in s3_issues:
                  check_id = issue.get("check_id", "")
                  file_path = issue.get("file_path", "")
                  resource = issue.get("resource", "")
                  
                  summary += f"""### {S3_PUBLIC_CHECKS.get(check_id, 'Unknown Check')}
          - **Check ID:** `{check_id}`
          - **File:** `{file_path}`
          - **Resource:** `{resource}`
          
          """
              
              summary += """
          ## ðŸ“‹ Manual Steps Required
          
          1. **Review the fixed files** in the `fixed_terraform/` directory
          2. **Test the changes** in a non-production environment
          3. **Replace your original files** with the fixed versions
          4. **Run terraform plan** to see the changes
          5. **Apply the changes** with `terraform apply`
          6. **For MFA delete**: Configure MFA delete manually via AWS CLI or Console
          
          ## âš ï¸ Important Notes
          
          - **Access Logging**: Creates additional S3 buckets for storing access logs
          - **MFA Delete**: Requires MFA device configuration and can only be enabled by root user
          - **Encryption**: Uses AES256 by default, consider KMS for enhanced security
          - **Cost Impact**: Additional resources may incur extra AWS charges
          
          ---
          *Generated by S3 Security Gatekeeper*
          """
              
              return summary
          
          def main():
              print("ðŸ”§ Starting S3 fixes generation...")
              
              data = load_checkov_report()
              if not data:
                  print("No Checkov data found")
                  return
              
              failed_checks = parse_checkov_data(data)
              s3_issues = identify_s3_issues(failed_checks)
              
              if not s3_issues:
                  print("âœ… No S3 issues to fix")
                  return
              
              print(f"ðŸ” Found {len(s3_issues)} S3 issues to fix")
              
              # Create fixed directory
              fixed_dir = Path("fixed_terraform")
              fixed_dir.mkdir(exist_ok=True)
              
              files_processed = set()
              
              for issue in s3_issues:
                  file_path = issue.get("file_path", "")
                  if not file_path or not os.path.exists(file_path):
                      continue
                  
                  if file_path in files_processed:
                      continue
                  
                  print(f"ðŸ”§ Processing {file_path}")
                  
                  try:
                      with open(file_path, 'r') as f:
                          content = f.read()
                      
                      original_content = content
                      
                      # Apply all fixes for this file
                      file_issues = [i for i in s3_issues if i.get("file_path") == file_path]
                      for file_issue in file_issues:
                          resource_name = file_issue.get("resource", "")
                          check_id = file_issue.get("check_id", "")
                          content = fix_terraform_content(content, resource_name, check_id)
                      
                      # Save fixed file if changes were made
                      if content != original_content:
                          fixed_file_path = fixed_dir / Path(file_path).name
                          with open(fixed_file_path, 'w') as f:
                              f.write(content)
                          print(f"âœ… Fixed file saved: {fixed_file_path}")
                          files_processed.add(file_path)
                      
                  except Exception as e:
                      print(f"âŒ Error processing {file_path}: {e}")
              
              # Generate fix summary
              fix_summary = generate_fix_summary(s3_issues, files_processed)
              with open("fixed_terraform/fix_summary.md", "w") as f:
                  f.write(fix_summary)
              
              print(f"ðŸŽ¯ Processed {len(files_processed)} files")
              print("ðŸ“„ Fix summary generated: fixed_terraform/fix_summary.md")
          
          if __name__ == "__main__":
              main()
          EOF
          
          python scripts/fix_s3_buckets.py

      - name: ðŸ“¤ Upload Fixed Files
        uses: actions/upload-artifact@v4
        with:
          name: s3-security-fixes
          path: |
            fixed_terraform/
            checkov_reports/
          retention-days: 30

      - name: ðŸ’¬ Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            try {
              const summaryPath = 'checkov_reports/security_summary.md';
              if (fs.existsSync(summaryPath)) {
                const summary = fs.readFileSync(summaryPath, 'utf8');
                
                await github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: `## ðŸ›¡ï¸ S3 Security Scan Results\n\n${summary}\n\n### ðŸ“ Artifacts\n- **Fixed Files**: Available in the workflow artifacts\n- **Full Report**: Check the \`s3-security-fixes\` artifact`
                });
              }
            } catch (error) {
              console.log('Error posting comment:', error);
            }

  # Job 3: Create GitHub Release with Fixes (optional)
  create-release:
    name: ðŸ“¦ Create Security Fix Release
    runs-on: ubuntu-latest
    needs: [security-scan, generate-fixes]
    if: needs.security-scan.outputs.has-s3-issues == 'true' && github.ref == 'refs/heads/main'
    permissions:
      contents: write
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ“¥ Download Fixes
        uses: actions/download-artifact@v4
        with:
          name: s3-security-fixes
          path: security-fixes/

      - name: ðŸ“¦ Create Release Archive
        run: |
          cd security-fixes
          zip -r ../security-fixes.zip .
          cd ..

      - name: ðŸ·ï¸ Create Release
        id: create_release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: security-fix-${{ github.run_number }}
          release_name: ðŸ›¡ï¸ S3 Security Fixes - ${{ github.run_number }}
          body: |
            ## ðŸ›¡ï¸ Automated S3 Security Fixes
            
            This release contains automatically generated fixes for S3 security issues detected in the codebase.
            
            ### ðŸ“ Contents
            - **Fixed Terraform Files**: Secure versions of your infrastructure code
            - **Security Reports**: Detailed analysis of issues found and fixed
            - **Fix Summary**: Step-by-step guide for applying the fixes
            
            ### ðŸš€ How to Apply
            1. Download the `security-fixes.zip` from this release
            2. Extract and review the fixed Terraform files
            3. Read the `fix_summary.md` for detailed instructions
            4. Replace your original files with the secure versions
            5. Run `terraform plan` to verify changes
            6. Apply the fixes with `terraform apply`
            
            **Generated:** ${{ github.run_id }}
            **Commit:** ${{ github.sha }}
          draft: false
          prerelease: false

      - name: ðŸ“¤ Upload Release Assets
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create_release.outputs.upload_url }}
          asset_path: ./security-fixes.zip
          asset_name: security-fixes.zip
          asset_content_type: application/zip

  # Job 4: Security Status Check
  security-status:
    name: ðŸŽ¯ Security Status
    runs-on: ubuntu-latest
    needs: [security-scan, generate-fixes]
    if: always()
    
    steps:
      - name: ðŸ“Š Report Final Status
        run: |
          echo "## ðŸ›¡ï¸ S3 Security Gatekeeper Status"
          echo "- **Scan Status**: ${{ needs.security-scan.outputs.scan-status }}"
          echo "- **S3 Issues Found**: ${{ needs.security-scan.outputs.has-s3-issues }}"
          echo "- **Fixes Generated**: ${{ needs.generate-fixes.result == 'success' }}"
          
          if [ "${{ needs.security-scan.outputs.has-s3-issues }}" == "true" ]; then
            echo "âŒ S3 security issues detected and fixes generated"
            exit 1
          else
            echo "âœ… No S3 security issues found"
            exit 0
          fi

  # Job 5: Slack/Teams Notification (optional)
  notify:
    name: ðŸ“¢ Send Notifications
    runs-on: ubuntu-latest
    needs: [security-scan, generate-fixes]
    if: always() && needs.security-scan.outputs.has-s3-issues == 'true'
    
    steps:
      - name: ðŸ“¢ Slack Notification
        if: env.SLACK_WEBHOOK_URL != ''
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: |
            ðŸš¨ S3 Security Issues Detected!
            
            Repository: ${{ github.repository }}
            Branch: ${{ github.ref_name }}
            Commit: ${{ github.sha }}
            
            Issues found and fixes generated automatically.
            Check the workflow artifacts for details.
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: ðŸ“§ Email Notification Summary
        run: |
          echo "## ðŸ“§ Notification Summary"
          echo "S3 security issues have been detected and processed:"
          echo "- Scan completed with issues found"
          echo "- Automatic fixes generated"
          echo "- Release created (if on main branch)"  
          echo "- Team notifications sent (if configured)"
          echo ""
          echo "Next steps:"
          echo "1. Review the generated fixes in workflow artifacts"
          echo "2. Apply the security improvements to your infrastructure"
          echo "3. Re-run the security scan to verify fixes"